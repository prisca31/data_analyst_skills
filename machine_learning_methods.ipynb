{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/prisca31/data_analyst_skills/blob/main/machine_learning_methods.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# This is one of the exercice of the certification, you can read the questions and my answers !"
      ],
      "metadata": {
        "id": "_X_rnGC-bcqz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Comparer les regressions"
      ],
      "metadata": {
        "id": "swfDug53bB4g"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Comparez le modèle de régression linéaire et le modèle de régression logistique. Pourquoi quelles tâches sont les plus adaptées ? Quels sont leurs avantages et leurs inconvénients ?"
      ],
      "metadata": {
        "id": "ar-nZ4qgbThA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ces deux modèles constituent des méthodes de \"Supervised Learning\", c'est à dire qu'on connait la target.\n",
        "\n",
        "Le modèle de régression linéaire est utilisé pour des variables continues alors que le modèles de régression logistique est utilisé pour classifier (prédire si la cible sera dans telle ou telle catégorie).\n",
        "\n",
        "Par exemple, la régression linéaire va être utilisée pour obtenir un chiffre : le prix de vente d'une maison à l'aide de variables (taille, nombre de pièces etc) alors que la régression logistique va être utilisée pour déterminer si un mail reçu est un spam ou encore si un patient à un cancer ou non (selon les caractéristiques définies).\n",
        "\n",
        "Ces deux modèles sont faciles à utiliser et à calculer. Cependant ils sont sensibles aux outliers (valeurs extrêmes). Pour la régression linéaire, pour l'utiliser sur des variables qualitatives il faut les encoder, ce qui est plus contraignant. En ce qui concerne la régression logistique, elle ne prend peut être appliquée pour des multiples catégories."
      ],
      "metadata": {
        "id": "Pv5M7_hPbNxw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Justification de l'accuracy"
      ],
      "metadata": {
        "id": "5MrU4y2GbaRd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Vous avez entraîné un modèle pour prédire si un produit aura du succès ou non. Vous obtenez une accuracy de 0.86. Êtes-vous satisfait ? Pourquoi ?"
      ],
      "metadata": {
        "id": "S-vlmDqUbioa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Je suis plutôt satisfaite car ce chiffre signifie que 86% des produits que je classe (que mon modèle classe) en \"produit qui aura du succès\" aura effectivement eu du succès ce qui est un bon score et consititue donc probablement une bonne prévision."
      ],
      "metadata": {
        "id": "r3z0_jWEbjgW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Différents pré-traitements"
      ],
      "metadata": {
        "id": "MgtYK3wSbnjS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Quels sont les différents pré-traitements de données que vous connaissez ? Précisez à quel type de données ils s’appliquent et le code Python si possible."
      ],
      "metadata": {
        "id": "hhoBeo1_bzfj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "En ce qui concerne les pré-traitements je dois d'abord m'assurer que mon dataset ne comporte pas de valeurs dupliquées mais surtout pas de valeurs nulles (je ne pourrais pas l'entrainer).\n",
        "\n",
        "Je dois sélectionner mes variables X et ma target y.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "10YHjUYob0pw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X = df[['var1', 'var2', 'var3']]\n",
        "y = df['var_target']"
      ],
      "metadata": {
        "id": "VPz17JIxY0dT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Je vais appliquer un \"train-test split\" ce qui signifie que je vais diviser mon dataset en deux parties : une partie qui servivra à l'entrainement de mon modèle et une partie qui servira à le tester. Par exemple 80% d'entrainement et 20% de test (test_size), et je vais choisir aléatoirement les valeurs qui compose chaque partie (random_state)."
      ],
      "metadata": {
        "id": "oyCjyoNuZGSK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "1T45f11KYv1C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Mais au préalable X et y doivent être des variables numériques, je dois donc les encoder si ce sont des variables non-numériques"
      ],
      "metadata": {
        "id": "BP2FbRYgZL1l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Pour X :\n",
        "numerical_features = X_train.select_dtypes(include=['number'])\n",
        "non_numerical_features =  X_train.select_dtypes(exclude=['number'])\n",
        "\n",
        "# Ou avec le nom de chaque variable à la place de include/exclude=['number']"
      ],
      "metadata": {
        "id": "ds8U5-uuY96E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ensuite j'encode les non_numerical_features : ce que je fais c'est que j'attribue des chiffres à chaque donnée unique de variables qualitatives: chaque valeur unique devient une colonne"
      ],
      "metadata": {
        "id": "1Tt7VQldZPPj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import OneHotEncoder\n",
        "\n",
        "enc = OneHotEnconder(handle_unknown='ignore', sparse_output=False)\n",
        "X_train_ohe = enc.fit_transform(X_train)\n",
        "X_test_ohe = enc.transform(X_test)\n",
        "\n",
        "enc.categories_"
      ],
      "metadata": {
        "id": "KH8K_DT0ZOpQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Si c'est mon Y qui n'est une variable qualitative je lui applique le LabelEnconder : chaque valeur unique obitent un chiffre"
      ],
      "metadata": {
        "id": "HUu8alFgaRji"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "enc = LabelEncoder()\n",
        "y_encoded = enc.fit_transform(y)\n",
        "\n",
        "enc.classes_"
      ],
      "metadata": {
        "id": "VQjUc1KpaPLY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Une fois les X non_numerical_values encodées, il faut assembler à nouveau les X non_numerical et les X numerical dans un même dataset pour pouvoir les traiter ensemble (je les concatène).\n",
        "\n",
        "Une fois fait, je dois mettre les variables à la \"même échelle\" pour qu'elles aient une \"même échelle de poids\" dans le modèle, j'utilise la méthode StandardScaler (je peux aussi utiliser d'autres méthodes par exemple RobustScaler selon les besoins) - c'est à appliquer sur le train et le test"
      ],
      "metadata": {
        "id": "hRXN_C8aayTP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)"
      ],
      "metadata": {
        "id": "p0BtxNsqaxst"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Les pré-traitements sont terminés. Je peux maintenant appliquer ma régression linéaire ou logistique."
      ],
      "metadata": {
        "id": "MqpL64JJb9gO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Ré-équilibrer la donnée"
      ],
      "metadata": {
        "id": "LpKwHUrnb6L4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Vous travaillez pour une plateforme de streaming de films. Vous voulez prédire si un film va être beaucoup regardé en fonction de ses caractéristiques.\n",
        "\n",
        "\n",
        "Votre jeu de données n’est pas balancé - vous avez 10% de vos films qui ont été peu regardés et 90% qui ont été beaucoup regardés.\n",
        "\n",
        "\n",
        "Que pouvez-vous faire pour rebalancer la donnée ? Plusieurs solutions sont possibles."
      ],
      "metadata": {
        "id": "7NZvStL5cEG4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Le problème est que le modèle va être biaisé et prédire les classes les plus représentés il faut donc rééquilibrer les données. On peut par exemple appliquer un sur-échantillonage de la classe la moins regarder ou un sous-échantilonage des classes les plus regardés de sorte à ce que chaque classe est le même coefficient. C'est à dire qu'on augmente le nombre de film sous  représenté (oversampling) ou on diminue le nombre de film sur représenté (undersampling)."
      ],
      "metadata": {
        "id": "D8jipzVXcHbT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Efficacité du modèle"
      ],
      "metadata": {
        "id": "cKpI9zIpcMT-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Comment savoir si le prétraitement des données a amélioré l’efficacité du modèle ?"
      ],
      "metadata": {
        "id": "FlWUhiCTcVC5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "On peut comparer le score obtenu. Par exemple en régression linéaire : score = lin_reg.score(X_test_scaled, y_test) en utilisant le X_train scaled et non scaled (de même pour le X_test). Si le score est plus cohérent c'est que le modèle est plus efficace grâce aux prétraitements.\n",
        "\n",
        "On peut aussi comparer le MAE (Mean Absolute Error), MSE (Mean Squarred Error) et le R2 (coefficient de détermination), si ces scores ce sont améliorés après l'application des pré-traitements c'est qu'ils ont eu un effet positif sur l'efficacité du modèle."
      ],
      "metadata": {
        "id": "EgJP9-59cXJ3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data leakage"
      ],
      "metadata": {
        "id": "vd7nY7zIcbcj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Comment éviter de faire du data leakage ? Ajoutez le code Python."
      ],
      "metadata": {
        "id": "RNA4lZDdclt_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Il faut bien séparer le X et le y, et donc s'assurer qu'on n'inclut pas le y dans le pre-processing.\n",
        "Le target NE DOIT PAS être dans le y, et on ne doit pas appliquer de pré-traitement de type \"scaling\" : il n'existe PAS de y_train_SCALED"
      ],
      "metadata": {
        "id": "AmbfMpN3cmnc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X = df.drop('target', axis=1) # j'exclue ma target\n",
        "y = df['target'] # je ne garde que ma target"
      ],
      "metadata": {
        "id": "7A-_mGNLcozS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "J'applique ma regresion linéaire à X_train_scaled et y_train, de même pour le X_test_scaled et y_test"
      ],
      "metadata": {
        "id": "CFToaMnpgr7e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "lin_reg = LinearRegression()\n",
        "lin_reg.fit(X_train_scaled, y_train)\n",
        "\n",
        "score = lin_reg.score(X_test_scaled, y_test)"
      ],
      "metadata": {
        "id": "KmebLndvgiNA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Enfin je peux appliquer une méthode de validation croisée ou cross_val_score. Elle permet de diviser mes données en plusieurs \"plis\" (k-fold -> déterminés par le cv) et entraine mon modèle autant de fois que de k-fold."
      ],
      "metadata": {
        "id": "pCZg-m1chXhP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import sklearn.model_selection from cross_val_score\n",
        "\n",
        "score = cross_val_score(estimate = lin_reg, X = X_train_scaled, y = y_train, cv =5)"
      ],
      "metadata": {
        "id": "D-lSjbYJhWYO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Etapes d'entraînement d'algorithme"
      ],
      "metadata": {
        "id": "860iXLv0crav"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Imaginez que vous devez entraîner un algorithme de régression linéaire.\n",
        "\n",
        "Quelles sont les différentes étapes nécessaires ?\n",
        "\n",
        "Utilisez le code Python et n’oubliez pas de préciser la bonne donnée entre donnée d’entraînement et donnée de test."
      ],
      "metadata": {
        "id": "VjE1CPL0c0Bu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ce n'est pas déjà tout ce que j'ai décrit ci-dessus ?\n",
        "J'y ajoute l'étape de prédiction...\n",
        "\n",
        "1.   Je sélectionne mon X et mon y\n",
        "2.   Je fais un train-test-split et j'encode mon X et/ou y si nécessaire (one-hot encoding ou label encoding)\n",
        "3.   Je scale mes variables (ex. standardscaler)\n",
        "4.   J'entraine mon modèle (ex. régression linéaire)\n",
        "5.   Je prédis avec un nouveau dataset X_new\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "mCNu9Tdkc3e3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = df[['var1', 'var2', 'var3', 'var_target']]\n",
        "\n",
        "# Toutes mes données sont numériques\n",
        "\n",
        "X = df[['var1', 'var2', 'var3']]\n",
        "y = df['var_target']\n",
        "\n",
        "# Train_test_split\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# StandardScaler\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# LinearRegression\n",
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "# -> j'entraine\n",
        "lin_reg = LinearRegression()\n",
        "lin_reg.fit(X_train_scaled, y_train)\n",
        "\n",
        "# -> j'évalue\n",
        "score = lin_reg.score(X_test_scaled, y_test)\n",
        "\n",
        "# Predict\n",
        "lin_reg.predict(X_new)"
      ],
      "metadata": {
        "id": "QHfM_v6ec6wg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hyper Paramètres"
      ],
      "metadata": {
        "id": "pnMuPp59dAAM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Imaginez que vous devez entraîner un KNNClassifier. Quels sont les différents hyper paramètres que vous pouvez modifier ?\n",
        "\n",
        "Choisissez-en trois et expliquez quels impacts ils auront."
      ],
      "metadata": {
        "id": "K1jq0S4MdLDG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Un KNNClassifier est une modèle qui permet de créer des catégories (clustering) selon la distance/poids de chaque point par rapport à ses voisins, c'est une méthode de \"unsupervised learning\". Pour l'appliquer, on peut utiliser le K-Means qui regroupent des points autour d'un centroid (le centre du cluster), chaque point est associé au centroid le plus proche de lui. Pour cela on doit définir le nombre de centroid que l'on veut et donc le nombre de groupe - avec l'aide de la Elbow-method.\n",
        "\n",
        "On peut aussi utiliser le DSSCAN qui trouve \"tout seul\" le nombre de centroid en fonction des paramètres qu'on lui fournit (le epsilon et min-samples -> densité et échantillons par rayon).\n",
        "\n",
        "En fin de compte il est possible de jouer sur des hyper paramètres tels que, entre autres, le nombre de voisin pour déterminer la classe (le nombre de points dont on va prendre les caractéristiques pour déterminer le modèle de la classe), la distance entre chaque voisin (pour déterminer si un point est à l'intérieur ou au contraire trop éloigné de la classe), ou encore le poids de chaque voisin (s'ils sont trop proches par exemple)."
      ],
      "metadata": {
        "id": "0T1-CbOmdOGy"
      }
    }
  ]
}